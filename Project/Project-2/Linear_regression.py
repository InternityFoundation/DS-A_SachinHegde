# -*- coding: utf-8 -*-
"""project S.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10usrGF6igt0GYsPg5bNWdP9leDeyvfXp
"""

#impport libraries
import pandas as pd
import matplotlib.pyplot as pt
import numpy as nm

#Reading Excel data
df = pd.read_excel("slr02.xls")
#Data in file slr02.xls
X=df.iloc[:,0]
Y=df.iloc[:,1]
print(df.head())
#plotting graph for given data
pt.style.use('seaborn')
pt.scatter(X,Y)
pt.show()
#plotting graph for given data with labels
pt.scatter(X,Y,)
pt.title("Cricket Chirps Vs. Temperature")
pt.xlabel("chirps/sec for the striped ground cricket")
pt.ylabel("temperature in degrees Fahrenheit")
pt.show()

"""---

**Using Ordinary Least Square method(OLS)**
"""

#Finding mean of Y
s=sum(Y)
n=len(Y)
y_mean=s/n
print(y_mean)
#Finding mean of X
x_mean=nm.mean(X)
print(x_mean)

#Finding slope and c values and new 'Y' values
m_num=0
m_den=0
for i in range(len(Y)):
  m_num+=((X[i]-x_mean)*(Y[i]-y_mean))
  m_den+=(X[i]-x_mean)**2
#print(m_num, m_den)
m=(m_num/m_den)
print("Slope=",m)
b=(y_mean-(m*x_mean))
print("c=",b)
print("The new line equation is Y=",round(m,2),"*X +",round(b,2))
Y_new=(m*X)+b

#Finding error rate before regression
error=0
error_after=0
for i in range(len(Y)):
  error+=((Y[i]-y_mean)**2)
  error_after+=(Y_new[i]-y_mean)**2
print(error)
print(error_after)

#Plotting old values as well as new values
pt.scatter(X,Y)
pt.plot([min(X), max(X)], [min(Y_new), max(Y_new)], color='red',label="Predicted values")
pt.legend()
pt.show()

"""---

**R square value for OLS method**
"""

#Finding R square values
R_square=(error_after/error)*100
#print(R_square)
print("R^2 value is =",R_square,"%")

"""---

**Gradient Descent Algorithm**
"""

sd_x=X.std()
x=(X-x_mean)/sd_x
pt.scatter(x,Y)
pt.title("Cricket Chirps Vs. Temperature")
pt.xlabel("chirps/sec for the striped ground cricket")
pt.ylabel("temperature in degrees Fahrenheit")
pt.show()

def hypothesis(x,theta):
    y_new = theta[0] + theta[1]*x
    return y_new
def gradient(X,Y,theta):
    m = X.shape[0]
    grad = nm.zeros((2,))
    for i in range(m):
        x = X[i]
        y_new = hypothesis(x,theta)
        y = Y[i]
        grad[0] += (y_new - y)
        grad[1] += (y_new - y)*x
    return grad/m
def error(X,Y,theta):
    m = X.shape[0]
    total_error = 0.0
    for i in range(m):
        y_new = hypothesis(X[i],theta)
        total_error += (y_new - Y[i])**2
        
    return (total_error/m)
def gradientDescent(X,Y,max_steps=500,learning_rate =0.1):
    
    theta = nm.zeros((2,))
    error_list = []
    theta_list = []
    
   
    for i in range(max_steps):
        
        # Compute grad
        grad = gradient(X,Y,theta)
        e = error(X,Y,theta)
        
        
        #Update theta
        theta[0] = theta[0] - learning_rate*grad[0]
        theta[1] = theta[1] - learning_rate*grad[1]
        # Storing the theta values during updates
        theta_list.append((theta[0],theta[1]))
        error_list.append(e)
        
    return theta,error_list,theta_list

theta,error_list,theta_list = gradientDescent(x,Y)
print(theta)
print(error_list)

pt.plot(error_list)
pt.title("Reduction of error over time")
pt.show()

y_new=hypothesis(x,theta)
print(y_new)

pt.scatter(x,Y)
pt.plot(x,y_new,color='red',label="Predicted values")
pt.legend()
pt.show()

"""---

**R square value for Gradient Descent algorithm**
"""

error_g=0
error_after_g=0
for i in range(len(Y)):
  error_g+=((Y[i]-y_mean)**2)
  error_after_g+=(y_new[i]-y_mean)**2
#print(error_g)
#print(error_after_g)
#Finding R square values
R_square_g=(error_after_g/error_g)*100
print("R^2 value is =",R_square_g,"%")